import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder
import os
from tqdm import tqdm

# ============================================
# 0) ì„¤ì •ê°’
# ============================================

DATASET = "/home/ice06/project/secure/hyewon/advice/dataset/Modbus_dataset/selected_ics_45719.csv"
OUTPUT_DIR = "/home/ice06/project/secure/mrmr_test/dataset/new_modbus/"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ë¼ë²¨ ì»¬ëŸ¼ (feature selectionì—ì„œ ì ˆëŒ€ í¬í•¨ë˜ë©´ ì•ˆ ë¨)
LABEL_COLS = [
    "NST_M_Label", "NST_B_Label",
    "IT_M_Label", "IT_B_Label"
]

# MISS íŒŒë¼ë¯¸í„°
N_BOOTSTRAP = 30 # ëª‡ ë²ˆ ì„ì„ì§€
SAMPLE_FRAC = 0.7 # ë§¤ë²ˆ ëª‡ ê°œ ë½‘ì„ì§€
TOP_K_EACH_ROUND = 30
PROB_THRESHOLD = 0.7
RANDOM_STATE = 42

# ============================================
# 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ============================================

print("ğŸ“‚ Loading dataset...")
df = pd.read_csv(DATASET).fillna(0)

print(f"- Total samples in dataset: {len(df)}")

# ============================================
# 2) ë¼ë²¨ ì»¬ëŸ¼ ì™„ì „ ì œê±° (ê°•í™” ë²„ì „)
# ============================================

# ëŒ€ì†Œë¬¸ì ë¬´ì‹œë¥¼ ìœ„í•œ lowercase ë¦¬ìŠ¤íŠ¸
LABEL_COLS_LOWER = [c.lower() for c in LABEL_COLS]

# df ë‚´ë¶€ì—ì„œ ë¼ë²¨ë¡œ ê°„ì£¼ë˜ëŠ” ì»¬ëŸ¼ ìë™ íƒì§€
detected_label_cols = [c for c in df.columns if c.lower() in LABEL_COLS_LOWER]

print("ğŸš« Excluding label columns from feature candidates:")
print(detected_label_cols)

# feature candidates = ë¼ë²¨ ì»¬ëŸ¼ ì œê±° + ìˆ«ìí˜•ë§Œ ë‚¨ê¸°ê¸°
feature_df = df.drop(columns=detected_label_cols, errors='ignore')
feature_df = feature_df.select_dtypes(include=[np.number])

# ë‹¤ì‹œ í•œ ë²ˆ ë¼ë²¨ì´ ë“¤ì–´ìˆëŠ”ì§€ ì•ˆì „ê²€ì‚¬
bad_cols = [c for c in feature_df.columns if c.lower() in LABEL_COLS_LOWER]
if len(bad_cols) > 0:
    raise ValueError(f"âŒ ERROR: label columns detected in feature set â†’ {bad_cols}")

feature_names = feature_df.columns.tolist()
X_np = feature_df.values

print(f"- Feature candidates after removal: {len(feature_names)}")

# ============================================
# 3) ë¼ë²¨ ì¸ì½”ë”© (NST_M_Label ê¸°ì¤€)
# ============================================

label_for_miss = "NST_M_Label"

if label_for_miss not in df.columns:
    raise ValueError("âŒ NST_M_Label is missing in the dataset!")

y_raw = df[label_for_miss].astype(str)
le = LabelEncoder()
y = le.fit_transform(y_raw)

# ============================================
# 4) MISS ì•Œê³ ë¦¬ì¦˜ ì •ì˜
# ============================================

def miss_feature_selection(
    X: np.ndarray,
    y: np.ndarray,
    feature_names,
    n_bootstrap=30,
    sample_frac=0.7,
    top_k_each_round=30,
    prob_threshold=0.7,
    random_state=42,
):
    n_samples, n_features = X.shape
    select_counts = np.zeros(n_features, dtype=int)
    mi_sums = np.zeros(n_features)

    rng = np.random.RandomState(random_state)

    print("ğŸš€ Running MISS (Mutual Information + Stability Selection)...")
    for b in tqdm(range(n_bootstrap)):
        idx = rng.choice(n_samples, size=int(n_samples * sample_frac), replace=True)
        X_b = X[idx]
        y_b = y[idx]

        mi = mutual_info_classif(
            X_b, y_b, 
            discrete_features=False,
            random_state=rng.randint(0, 999999)
        )

        top_k = min(top_k_each_round, n_features)
        top_idx = np.argsort(mi)[::-1][:top_k]

        select_counts[top_idx] += 1
        mi_sums += mi

    selection_prob = select_counts / n_bootstrap
    avg_mi = mi_sums / n_bootstrap

    result_df = pd.DataFrame({
        "feature": feature_names,
        "select_count": select_counts,
        "selection_prob": selection_prob,
        "avg_mi": avg_mi
    }).sort_values(
        by=["selection_prob", "avg_mi"], ascending=[False, False]
    ).reset_index(drop=True)

    selected_df = result_df[result_df["selection_prob"] >= prob_threshold]

    if len(selected_df) == 0:
        print("âš ï¸ No feature meets threshold â†’ fallback to top 20")
        selected_df = result_df.head(20)

    return selected_df["feature"].tolist(), result_df, selected_df

# ============================================
# 5) MISS ì‹¤í–‰
# ============================================

selected_features, result_df, selected_df = miss_feature_selection(
    X_np, y, feature_names,
    n_bootstrap=N_BOOTSTRAP,
    sample_frac=SAMPLE_FRAC,
    top_k_each_round=TOP_K_EACH_ROUND,
    prob_threshold=PROB_THRESHOLD,
    random_state=RANDOM_STATE
)

print("\nâœ… MISS selected features:")
for f in selected_features:
    print(" -", f)

print(f"\nì´ ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(selected_features)}")

# ============================================
# 6) ê²°ê³¼ ì €ì¥
# ============================================

result_df.to_csv(os.path.join(OUTPUT_DIR, "miss_feature_scores.csv"), index=False)
selected_df.to_csv(os.path.join(OUTPUT_DIR, "miss_selected_features.csv"), index=False)

# ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±: ì„ íƒëœ Feature + 4ê°œ Label ì»¬ëŸ¼ ìœ ì§€
df_reduced = df[selected_features + LABEL_COLS]
df_reduced.to_csv(os.path.join(OUTPUT_DIR, "Dataset_miss_selected.csv"), index=False)

print("ğŸ’¾ Saved Dataset_miss_selected.csv")
